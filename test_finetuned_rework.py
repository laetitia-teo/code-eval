print("aaaaaaa")


import argparse
import copy
import os
# from key import wandb_key   
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM,TrainingArguments,LlamaTokenizer

from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
from datasets import load_dataset,concatenate_datasets
from tqdm import tqdm
import json
from datasets import Dataset

# from peft import LoraConfig
import numpy as np
from utils_test import return_full_prompt,prompt_train,pass_at_k,Prompt_Intstruction,judge_parallel,preprocessing_P3_no_test,just_remove_example_in_docstring

parser = argparse.ArgumentParser(description="Example script for argument parsing")
parser.add_argument("-z", "--base_path", type=str, help="path to git project evaluate_model", default="/media/data/flowers/evaluate_model/")
parser.add_argument("--path_model_base", type=str, help="path where hf model are saved", default="/gpfsscratch/rech/imi/usv81js/hf/")

# parser.add_argument("-p", "--path_dir", type=str, help="path to evaluate_model")

# parser.add_argument("-p", "--arg_path_idx", type=int, help="path baseline idx  (data to use as trainset)", default=0)
parser.add_argument("-p", "--train_path", type=str, help="path of the trainset", default='puzzles_high_fitness_archivetrain.json')
# parser.add_argument("-p", "--test_path", type=int, help="path of the testset", default='puzzles_test.json')
parser.add_argument("-e", "--arg_epoch", type=int, help="number epoch", default=2)
parser.add_argument("-b", "--arg_bs", type=int, help=" bs train", default=8)
parser.add_argument("-c", "--arg_bs_test", type=int, help=" bs test", default=64)
parser.add_argument("-m", "--model_id", type=str, help=" model", default="deepseek-ai/deepseek-coder-1.3b-instruct")

parser.add_argument("-s", "--arg_seed", type=str, help="seed ", default="1")
parser.add_argument("-t", "--arg_mode", type=str, help=" model", default="train_eval")
parser.add_argument("-l", "--arg_sol", type=bool, help=" train just on sol or not", default=True)
parser.add_argument("-k", "--arg_k", type=int, help="k in pass@k", default=10)
parser.add_argument("-g", "--arg_gpu", type=str, help="GPU use", default="a100")
parser.add_argument("-a", "--accum_step", type=int, help="number of accumulation step", default=1)

args = parser.parse_args()
if args.arg_gpu == "v100":
    type_use = torch.float16
    bf16=False
    fp16=True
else:
    type_use = torch.bfloat16
    bf16=True
    fp16=False

accum_step=args.accum_step
# /!\ set that
# limited_trainset=True # data generated by expe with 3 first example from trainset or full trainset



if args.arg_mode=="train_eval":
    train_model=True
    eval_model=True
    
if args.arg_mode=="train":
    train_model=True
    eval_model=False 
if args.arg_mode=="eval":
    train_model=False
    eval_model=True 

if args.arg_epoch:
    num_train_epochs= args.arg_epoch

seed = str(args.arg_seed)



# test model

os.environ["WANDB_DISABLED"] = "True"

os.environ['TRANSFORMERS_OFFLINE'] = "1"
os.environ['WANDB_MODE'] = "offline"
os.environ["WANDB_PROJECT"] = "codegpt finetuned"
# os.environ['WANDB_API_KEY'] = wandb_key
os.environ['WANDB_CACHE_DIR'] = args.base_path+"wandb_cache/"


# os.environ['HF_DATASETS_CACHE'] = args.base_path+"hf/datasets"
# os.environ['TRANSFORMERS_CACHE'] = args.base_path+"hf/models"

os.environ['TOKENIZERS_PARALLELISM'] = "True"



# path_train_idx = args.arg_path_idx

# define all path

if seed[0]=="1" or seed[0]=="2" or seed[0]=="3":
    path_rd_gen= "run_saved/maps_"+seed[0]+"_rd_gen.json"
    path_elm="run_saved/maps_"+seed[0]+"_elm.json"
    path_elm_nlp="run_saved/maps_"+seed[0]+"_elm_NLP.json"
    path_img_rd="run_saved/maps_"+seed[0]+"_imgep_random.json"
    path_imgp_smart="run_saved/maps_"+seed[0]+"_imgep_smart.json"



list_name=["rd_gen","elm","elm_NLP","imgep_random","imgep_smart"]

list_all_path=[path_rd_gen, path_elm,path_elm_nlp,path_img_rd,path_imgp_smart]
path_train = args.base_path 

# curr_train_path = list_all_path[path_train_idx]
path_train += args.train_path
# name = list_name[path_train_idx]
name = args.train_path
path_save=args.base_path+"save_results/" + name + ".json"
print("\n=============\n ")

print("path train ", path_train)
print("\n=============\n ")

run_name_wandb=path_train.split(".json")[0].split('/')[-1]
print(f'run_name_wandb {run_name_wandb}')
# run_name_wandb = run_name_wandb.replace("/","_")
print(path_train)

# hf way to load json dataset
with open(path_train, encoding="utf-8") as f:
    dataset = json.load(f)
to_remove=["emb","target_skills","puzzle_history","quality","description","is_valid","is_valid_explanation"]
for i in dataset:
    for j in to_remove:
        if j in i:
            del i[j]
from datasets import Dataset

dataset = Dataset.from_list(dataset)

# dataset = load_dataset("json", data_files=path_train, trust_remote_code=True)#, split="train")
# dataset = load_dataset(path_train)#, trust_remote_code=True)#, split="train")

cat_datasets = dataset.shuffle(seed=42) 

model_id =   args.model_id

model_save_dir = 'save_models'

# hf_dir=args.path_model_base

# path_load_model=hf_dir+model_id
# print("path_load_model",path_load_model)


name_json_save_all = args.base_path + "save_results/passk.json"#.split("/")[1]
run_name = name + "e_"+str(num_train_epochs)+"_"+str(seed)
if not os.path.exists(name_json_save_all):
    # Create a new JSON file with some sample data
    sample_data = {}
    with open(name_json_save_all, 'w') as file:
        json.dump(sample_data, file, indent=4)


name_json = args.base_path+"save_results/" + name + '_' + model_id.split("/")[-1]
name_json_sol = args.base_path + "save_sol/" + name + '_' + model_id.split("/")[-1]

if not os.path.exists('save_results'):
    os.makedirs('save_results')
if not os.path.exists('save_sol'):
    os.makedirs('save_sol')

if train_model:
    tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=False)

    tokenizer.padding_side='right'
    tokenizer.pad_token = tokenizer.eos_token


    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        # torch_dtype=type_use,
        # quantization_config=quantization_config,
        device_map="auto",
        local_files_only=True
    )
    #training
    def formatting_prompts_func(example,prompt_solve_puzzle=prompt_train):
        output_texts = []
        # print(len(example['program_str']))
        for i in range(len(example['program_str'])):

            puzzle= example['program_str'][i]
            try:
                prompt_f=puzzle.split("def g(")[0]
                prompt_g= "def g(" + puzzle.split("def g(")[1]
                full_prompt = prompt_train.format(pb=prompt_f,g=prompt_g)#,g_firstline=prompt_g)
                output_texts.append(full_prompt)
            except:
                print("error in formatting_prompts_func idx",i)
                print(example['program_str'][i])
                print("======================")
                print(puzzle)
        return output_texts

    lr_scheduler_type= "cosine"

    warmup_ratio=0.1

    if args.arg_sol==True:
    # response_template= "Solution 1:" # for llama
        response_template= "Solution 1:"
    else:
        response_template = "Problem 1:"#"Solution 1:"

    # list_tok_response_template=tokenizer(response_template)["input_ids"][1:]

    collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer,mlm=False)

    run_name_wandb += "Epoch"+str(num_train_epochs)
    run_name_wandb += model_id.split("/")[-1]

    learning_rate=1e-5

    training_arguments=TrainingArguments(
        per_device_train_batch_size=args.arg_bs,
        # per_device_eval_batch_size=4,
        # evaluation_strategy="steps",
        gradient_accumulation_steps=accum_step,
        run_name= run_name_wandb,
        # warmup_steps=2,
        save_strategy="no",
        warmup_ratio=warmup_ratio,
        lr_scheduler_type=lr_scheduler_type,
        # max_steps=500,
        num_train_epochs=num_train_epochs,
        # weight_decay=0.001,
        learning_rate=learning_rate,
        bf16=bf16, 
        fp16=fp16,
        # bf16_full_eval=True,
        gradient_checkpointing=False,
        logging_steps=1,
        output_dir="outputs",
        optim="adamw_torch",#"paged_adamw_32bit",
        max_grad_norm=0.3,
        # group_by_length=True,
        # do_eval=True,
        # eval_steps=10,
        # torch_compile=True
        
    )

    config = {"lr":learning_rate, "batch_size": args.arg_bs,"warmup_ratio":warmup_ratio,"model_name":"model_id", "epoch":args.arg_epoch}
    # config.update({"architecture": "", "depth": 34})
    # wandb.init(#name=name_wb,
    #            config=config,project="finetune llama")


    trainer = SFTTrainer(
        model,#"EleutherAI/gpt-neo-125m",
        tokenizer=tokenizer,
        train_dataset=cat_datasets,
        # eval_dataset=dataset_r["test"],
        # dataset_text_field="program_str",

        formatting_func=formatting_prompts_func,
        data_collator=collator,
        max_seq_length=1024,
        args=training_arguments

    )
    trainer.train()

    output_dir = os.path.join(model_save_dir, f'{model_id.split("/")[-1]}_{run_name}') #args.base_path+"hf/datasets"+name # where to save model
    trainer.save_model(output_dir)

if eval_model:  # OOD

    output_dir = os.path.join(model_save_dir, f'{model_id.split("/")[-1]}_{run_name}')
    if train_model:
        trainer.accelerator.clear()
        import gc
        del model
        del tokenizer
        gc.collect()
        torch.cuda.empty_cache()
        for obj in gc.get_objects():
            if torch.is_tensor(obj):
                obj.cpu()            
        gc.collect()
        torch.cuda.empty_cache()

    if not train_model:
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        for obj in gc.get_objects():
            if torch.is_tensor(obj):
                obj.cpu()            
        gc.collect()
        torch.cuda.empty_cache()


    # testing
    tokenizer = AutoTokenizer.from_pretrained(output_dir, local_files_only=True)
    model = AutoModelForCausalLM.from_pretrained(
        output_dir,
        torch_dtype=type_use,

        # quantization_config=quantization_config,
        device_map="auto",
        local_files_only=True
    )
    tokenizer.padding_side='left'
    tokenizer.pad_token = tokenizer.eos_token
    model.eval()
    model.config.use_cache = True
    # model=torch.compile(model)

    # testset= preprocessing_P3_no_test(split="test",n_token_max=1024,path=args.base_path,tokenizer=tokenizer)
    path_test=args.base_path+"P3_test_emb_wizard3B.json"#"/home/flowers/work/OpenELM/old/run/P3_test_emb_codet5p.json"
    with open(path_test) as f:
        testset = json.load(f)
    curr_idx = 900
    correct_puzz = 0

    num_return_sequences=args.arg_k #n_try
    list_all_passk=[[] for i in range(num_return_sequences)]
    list_passk=[]

    list_puzzle=[]
    list_all_puzzle=[]

        
    list_testset= [x["program_str"] for x in testset]
    list_puzzle_correct=[]

    bs = args.arg_bs_test
    with torch.inference_mode():
        
        for idx in tqdm(range(curr_idx, len(list_testset),bs)): #len(dataset["test"])
            # idx=0
            print(f"\n\n============ idx {idx} ==================\n")
            flag=True
            attempt=0
            list_puzzle_idx=[]
            list_prompt=[]
            list_prompt_f=[]
            subset_test = list_testset[idx:idx+bs]
            for idx_puz in range(len(subset_test)):
                prompt_f = subset_test[idx_puz].split("def g(")[0]
                list_prompt_f.append(prompt_f)
                prompt = return_full_prompt(model_id=model_id,pb=prompt_f) # todo
                list_prompt.append(prompt)
            inputs = tokenizer(list_prompt, return_tensors="pt",padding=True).to("cuda")
            # for idx_inp in range(len(inputs)):
            len_prompt = inputs["input_ids"].shape[1]
            list_puzzle_gen=[[] for _ in range(len(list_prompt))]
            for idx_gen in range(num_return_sequences):
                outputs = model.generate(**inputs,max_new_tokens=512,do_sample=True, temperature=0.7)
                generated_texts = tokenizer.batch_decode(outputs[:,len_prompt:], skip_special_tokens=True)
                for idx_out_gen in range(len(outputs)):
                    list_puzzle_gen[idx_out_gen].append(generated_texts[idx_out_gen])

            list_generated_text = copy.deepcopy(list_puzzle_gen)

            for i in range(len(list_puzzle_gen)): # along the bs
                dic_save={}
                list_raw_puzzle = []
                list_proc_puzzle =[]
                for j in range(len(list_puzzle_gen[i])):
                    prompt_f =list_prompt_f[i]
                    try:
                        #check if "```" is in list_puzzle_gen[i][j]
                        list_puzzle_gen[i][j] = list_puzzle_gen[i][j].replace("```python","```")
                        list_puzzle_gen[i][j] = list_puzzle_gen[i][j].replace("```Python","```")

                        if "```" in list_puzzle_gen[i][j]:
                            extract_g=list_puzzle_gen[i][j].split("```")[1].split("assert")[0]
                        else:
                            if "assert" in list_puzzle_gen[i][j]:
                                extract_g=list_puzzle_gen[i][j].split("assert")[0]
                            else:    
                                extract_g=list_puzzle_gen[i][j]
                    except:
                        print("error extract g")
                        print(list_puzzle_gen[i][j])
                    extract_g = extract_g+"\nassert f(g()) == True\n"
                    test_fg= prompt_f+extract_g 
                    list_puzzle_gen[i][j] = test_fg
                    list_puzzle.append(test_fg)
                    list_proc_puzzle.append(test_fg)
                    list_raw_puzzle.append(prompt_f+list_puzzle_gen[i][j])
                dic_save["raw_puzzle"]=list_raw_puzzle
                dic_save["process_puzzle"]=list_proc_puzzle
                
                    # if j<1:
                    #     print("\n-------------------\n")
                    #     print(test_fg)
                    
                
                list_valid_puzzles = judge_parallel(list_puzzle_gen[i])
                dic_save["list_valid"]=list_valid_puzzles                 
                list_all_puzzle.append(dic_save)    

                cor_puz= np.sum(list_valid_puzzles)

                n_sample, n_correct=num_return_sequences,cor_puz
                pass_k = pass_at_k(n_sample, n_correct, k=num_return_sequences)
                list_passk.append(pass_k)
                #compute passk for k=[1,...,num_return_sequences]
                for idx_passk in range(num_return_sequences):
                    pass2add=pass_at_k(n_sample, n_correct, k=idx_passk+1)
                    list_all_passk[idx_passk].append(pass2add)
                    testset[idx + i][f'pass_{idx_passk+1}'] = pass2add

                proba_solved = n_correct / n_sample
                testset[idx + i]['proba_solved'] = float(proba_solved)
                testset[idx + i]['n_sample'] = int(n_sample)
                testset[idx + i]['n_correct'] = int(n_correct)
                testset[idx + i]['generated_text'] = list_generated_text[i]
                testset[idx + i]['parsed_puzzles'] = list_puzzle_gen[i]
                # testset[idx + i]['prompt'] = list_prompt[i]

                
            print(f"correct puzzles: {int(np.sum(list_passk))}/{len(list_passk)}")
            # with open(name_json+".json", "w") as outfile:
            #     json.dump(list_passk,outfile)

        for idx_passk in range(num_return_sequences):
            print(f"pass {idx_passk+1}: {np.sum(list_all_passk[idx_passk])}/{len(list_all_passk[idx_passk])}")
        dic_passk={}
        for idx_passk in range(num_return_sequences):
            dic_passk[f"pass_{idx_passk+1}"]=float(np.sum(list_all_passk[idx_passk]))

        with open(name_json_save_all, "r") as outfile:
            json_content = json.load(outfile)
        json_content[run_name] = dic_passk 
        with open(name_json_save_all, "w") as outfile:
            json.dump(json_content, outfile, indent=4)

        # with open(name_json+"_e"+str(num_train_epochs)+"_seed_"+seed+".json", "w") as outfile:
        #     json.dump(json_content,outfile,indent=4)
        # with open(name_json_sol+"_e"+str(num_train_epochs)+"_seed_"+seed+".json", "w") as outfile:
        #     json.dump(list_all_puzzle,outfile,indent=4)

        with open(name_json_sol + "_e" + str(num_train_epochs) + "_seed_" + seed + ".json", "w") as outfile:
            json.dump(testset, outfile, indent=4)

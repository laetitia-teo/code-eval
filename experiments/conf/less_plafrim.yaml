metric:
  name: less
  model_id_or_path: 'Qwen/Qwen1.5-0.5B'
  # model_id_or_path: 'deepseek-ai/deepseek-coder-1.3b-instruct'
  archive_path_or_list: /projets/flowers2/laetitia/code-eval/data/puzzles_train_1.json
  solution_mask: true
  batch_size: 2
  max_len: 1024

training:
  seed: 42
  log_level: INFO
  local_rank: -1  # not used
  device: cuda  # not used
  n_gpu: 1  # not used
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  run_name: less_dev_warmup
  save_strategy: 'steps'
  save_steps: 5
  # warmup_ratio: 0.1
  lr_scheduler_type: 'cosine'
  num_train_epochs: 2
  learning_rate: 1e-5
  bf16: true
  fp16: false
  gradient_checkpointing: false
  logging_steps: 1
  output_dir: outputs/less_dev
  optim: adamw_torch
  max_grad_norm: 0.3
  dev: false

model:
  # model_name_or_path: 'deepseek-ai/deepseek-coder-1.3b-instruct'
  model_name_or_path: 'Qwen/Qwen1.5-0.5B'
  config_name: null  # not used
  tokenizer_name: null  # not used
  cache_dir: None  # not used
  use_fast_tokenizer: false  # not used
  model_revision: main  # not used
  use_auth_token: false  # not used
  torch_dtype: bfloat16
  lora: true
  lora_r: 128
  lora_alpha: 512
  lora_dropout: 0.1
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

grad:
  info_type: grads
  model_path: outputs/less_dev
  max_samples: null
  torch_dtype: bfloat16
  output_path: outputs/less_dev/grads
  data_dir: null
  gradient_projection_dimension: [8192]
  gradient_type: adam
  max_length: 2048
  initialize_lora: false
  lora_r: 128
  lora_alpha: 512
  lora_dropout: 0.1
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

influence:
  train_file_names: ['data/p3_pb_dataset.json']
  checkpoint_weights: null  # detect automatically

dataset:
  name: p3
  path: /projets/flowers2/laetitia/code-eval/data/dataset.json
  overwrite_cache: false
  preprocessing_num_workers: null
  max_seq_length: 2048
  percentage: 1.0
  use_chat_format: yes

save_every: 50